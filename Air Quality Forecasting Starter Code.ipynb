{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTsEYdtov6tp"
   },
   "source": [
    "# Beijing Air Quality Forecasting Starter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWkSHhqXrCqF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxW-6b_jrLAL"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRse3uqRrft5"
   },
   "source": [
    "# Explore the training data\n",
    "\n",
    "In this sections explore your dataset with appropiate statistics and visualisations to understand your better. Ensure that you explain output of every code cell and what it entails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "3R74CEBFrYok",
    "outputId": "0e593627-9c80-490c-826e-74e4df4a2249"
   },
   "outputs": [],
   "source": [
    "print(\"Training Data Overview:\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-om6hH_RtG8Z",
    "outputId": "8fefc873-d80f-4b45-ead2-89bbfc8d4d62"
   },
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35IGrMYRscQx"
   },
   "outputs": [],
   "source": [
    "# Ensure 'datetime' column is in datetime format\n",
    "train['datetime'] = pd.to_datetime(train['datetime'])\n",
    "\n",
    "test['datetime'] = pd.to_datetime(test['datetime'])\n",
    "\n",
    "# Set the 'datetime' column as the index for better time-series handling\n",
    "train.set_index('datetime', inplace=True)\n",
    "# val.set_index('datetime', inplace=True)\n",
    "test.set_index('datetime', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_features(df):\n",
    "    df = df.copy()\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day'] = df.index.day\n",
    "    df['month'] = df.index.month\n",
    "    df['year'] = df.index.year\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    \n",
    "    # Cyclical encoding for periodic features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = create_temporal_features(train)\n",
    "test = create_temporal_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # For weather-related features, use interpolation\n",
    "    weather_cols = ['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']\n",
    "    for col in weather_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].interpolate(method='time')\n",
    "    \n",
    "    # Forward fill then backward fill for remaining\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = handle_missing_values(train)\n",
    "test = handle_missing_values(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pm2.5' in train.columns:\n",
    "    Q1 = train['pm2.5'].quantile(0.25)\n",
    "    Q3 = train['pm2.5'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Cap outliers instead of removing them\n",
    "    train['pm2.5'] = train['pm2.5'].clip(lower=max(0, lower_bound), upper=upper_bound)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (30598, 91)\n",
      "Test data shape: (13148, 28)\n"
     ]
    }
   ],
   "source": [
    "def create_advanced_features(df, target_col='pm2.5'):\n",
    "    \"\"\"Create more sophisticated features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if target_col in df.columns:\n",
    "        # Interaction features between weather variables\n",
    "        df['temp_dewp_interaction'] = df['TEMP'] * df['DEWP']\n",
    "        df['temp_pres_interaction'] = df['TEMP'] * df['PRES']\n",
    "        df['wind_temp_interaction'] = df['Iws'] * df['TEMP']\n",
    "        \n",
    "        # Weather condition categories\n",
    "        df['high_pressure'] = (df['PRES'] > df['PRES'].quantile(0.75)).astype(int)\n",
    "        df['low_pressure'] = (df['PRES'] < df['PRES'].quantile(0.25)).astype(int)\n",
    "        df['high_wind'] = (df['Iws'] > df['Iws'].quantile(0.75)).astype(int)\n",
    "        df['low_wind'] = (df['Iws'] < df['Iws'].quantile(0.25)).astype(int)\n",
    "        \n",
    "        # Seasonal patterns\n",
    "        df['winter'] = df['month'].isin([12, 1, 2]).astype(int)\n",
    "        df['spring'] = df['month'].isin([3, 4, 5]).astype(int)\n",
    "        df['summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "        df['autumn'] = df['month'].isin([9, 10, 11]).astype(int)\n",
    "        \n",
    "        # Rush hour indicators\n",
    "        df['morning_rush'] = df['hour'].isin([7, 8, 9]).astype(int)\n",
    "        df['evening_rush'] = df['hour'].isin([17, 18, 19]).astype(int)\n",
    "        df['weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        # Advanced lag features with decay\n",
    "        weights = np.exp(-np.arange(1, 13) * 0.1)  # Exponential decay\n",
    "        for i, weight in enumerate(weights, 1):\n",
    "            if f'{target_col}_lag{i}' in df.columns:\n",
    "                df[f'{target_col}_weighted_lag{i}'] = df[f'{target_col}_lag{i}'] * weight\n",
    "        \n",
    "        # Trend features\n",
    "        for window in [6, 12, 24]:\n",
    "            if f'{target_col}_rolling_mean_{window}' in df.columns:\n",
    "                df[f'{target_col}_trend_{window}'] = (\n",
    "                    df[target_col] - df[f'{target_col}_rolling_mean_{window}']\n",
    "                )\n",
    "        \n",
    "        # Fourier features for periodicity\n",
    "        for period in [24, 168, 8760]:  # Daily, weekly, yearly\n",
    "            df[f'sin_{period}'] = np.sin(2 * np.pi * df['hour'] / period)\n",
    "            df[f'cos_{period}'] = np.cos(2 * np.pi * df['hour'] / period)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# train = create_advanced_features(train)\n",
    "\n",
    "\n",
    "\n",
    "# # For test set, if it doesn't have pm2.5, use last known values from train\n",
    "# if 'pm2.5' not in test.columns:\n",
    "#     # Use the last few values from training data to initialize test features\n",
    "#     last_train_values = train['pm2.5'].tail(24).values\n",
    "#     test = create_enhanced_lag_features(test)\n",
    "#     # Fill initial NaN values with appropriate values\n",
    "#     for col in test.columns:\n",
    "#         if 'pm2.5' in col and test[col].isna().all():\n",
    "#             test[col] = np.mean(last_train_values)\n",
    "# else:\n",
    "#     test = create_enhanced_lag_features(test)\n",
    "\n",
    "# # Drop rows with NaN values (mainly from lag features)\n",
    "# train = train.dropna()\n",
    "# test = test.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# print(f\"Training data shape after feature engineering: {train.shape}\")\n",
    "# print(f\"Test data shape after feature engineering: {test.shape}\")\n",
    "\n",
    "# train = train.dropna()\n",
    "# test = test.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "\n",
    "def add_lag_features(df, target_col=\"pm2.5\", lags=[1, 6, 12, 24]):\n",
    "    df = df.copy()\n",
    "    for lag in lags:\n",
    "        df[f\"{target_col}_lag{lag}\"] = df[target_col].shift(lag)\n",
    "    return df\n",
    "\n",
    "# 1. Add lag features first (safe)\n",
    "train = add_lag_features(train, target_col=\"pm2.5\", lags=[1,6,12,24])\n",
    "test = add_lag_features(test, target_col=\"pm2.5\", lags=[1,6,12,24])\n",
    "\n",
    "# Drop NA rows in training\n",
    "train = train.dropna()\n",
    "\n",
    "# Fill forward/backward in test\n",
    "test = test.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# 2. Now create advanced features using those lag cols\n",
    "train = create_advanced_features(train, target_col=\"pm2.5\")\n",
    "test = create_advanced_features(test, target_col=\"pm2.5\")\n",
    "\n",
    "print(f\"Training data shape: {train.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABAqt0Jztd5s"
   },
   "source": [
    "# Handle missing values\n",
    "\n",
    "\n",
    "- Check the dataset for missing values and decide how to handle them.\n",
    "- In this example, missing values are filled with the mean. You can experiment with other strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "u2n29Ge1tami"
   },
   "outputs": [],
   "source": [
    "# ---- Handle missing values using forward fill ----\n",
    "# First, forward fill to carry previous values down\n",
    "train.fillna(method='ffill', inplace=True)\n",
    "test.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Backfill as a safety net (for first few rows which may still be NaN)\n",
    "train.fillna(method='bfill', inplace=True)\n",
    "test.fillna(method='bfill', inplace=True)\n",
    "\n",
    "\n",
    "# ---- Create lag features ----\n",
    "n_lags = 3\n",
    "\n",
    "for lag in range(1, n_lags + 1):\n",
    "    train[f'pm2.5_lag{lag}'] = train['pm2.5'].shift(lag)\n",
    "\n",
    "# If test set doesn't have pm2.5, give 0 as placeholder, else shift\n",
    "for lag in range(1, n_lags + 1):\n",
    "    test[f'pm2.5_lag{lag}'] = test['pm2.5'].shift(lag) if 'pm2.5' in test.columns else 0\n",
    "\n",
    "# Drop any remaining NaNs caused by shifting\n",
    "train = train.dropna()\n",
    "test.fillna(0, inplace=True)  # Fill any possible NaNs in test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKndkdRuty1C"
   },
   "source": [
    "# Separate features and target\n",
    "\n",
    "- Feel free to trop any non-essential columns like that you think might not contribute to modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "QETLRAo_tvQH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning:\n",
      "X_train infinite values: 0\n",
      "X_val infinite values: 0\n",
      "X_train NaN values: 0\n",
      "X_val NaN values: 0\n",
      "Attempting feature selection...\n",
      " Selected 50 features out of 90\n",
      "Final feature set size: 50\n",
      "Features: ['DEWP', 'Iws', 'cbwd_NW', 'pm2.5_lag1', 'pm2.5_lag2', 'pm2.5_lag3', 'pm2.5_lag4', 'pm2.5_lag5', 'pm2.5_lag6', 'pm2.5_lag7', 'pm2.5_lag8', 'pm2.5_lag9', 'pm2.5_lag10', 'pm2.5_lag11', 'pm2.5_lag12', 'pm2.5_rolling_mean_3', 'pm2.5_rolling_std_3', 'pm2.5_rolling_max_3', 'pm2.5_rolling_min_3', 'pm2.5_rolling_mean_6', 'pm2.5_rolling_std_6', 'pm2.5_rolling_max_6', 'pm2.5_rolling_min_6', 'pm2.5_rolling_mean_12', 'pm2.5_rolling_max_12', 'pm2.5_rolling_min_12', 'pm2.5_rolling_mean_24', 'pm2.5_rolling_max_24', 'pm2.5_rolling_min_24', 'pm2.5_ewm_3', 'pm2.5_ewm_12', 'pm2.5_pct_change_24', 'high_wind', 'pm2.5_weighted_lag1', 'pm2.5_weighted_lag2', 'pm2.5_weighted_lag3', 'pm2.5_weighted_lag4', 'pm2.5_weighted_lag5', 'pm2.5_weighted_lag6', 'pm2.5_weighted_lag7', 'pm2.5_weighted_lag8', 'pm2.5_weighted_lag9', 'pm2.5_weighted_lag10', 'pm2.5_weighted_lag11', 'pm2.5_weighted_lag12', 'pm2.5_trend_6', 'pm2.5_trend_12', 'pm2.5_trend_24', 'pm2.5_lag24', 'pm2.5_log']\n",
      "\n",
      " Final data shapes:\n",
      "X_train_seq: (24452, 24, 50)\n",
      "y_train_seq: (24452,)\n",
      "X_val_seq: (6095, 24, 50)\n",
      "y_val_seq: (6095,)\n"
     ]
    }
   ],
   "source": [
    "val_size = int(len(train) * 0.2)\n",
    "train_data = train.iloc[:-val_size]\n",
    "val_data = train.iloc[-val_size:]\n",
    "\n",
    "# Separate features and target\n",
    "feature_cols = [col for col in train.columns if col not in ['pm2.5', 'No']]\n",
    "X_train = train_data[feature_cols].copy()\n",
    "y_train = train_data['pm2.5'].copy()\n",
    "X_val = val_data[feature_cols].copy()\n",
    "y_val = val_data['pm2.5'].copy()\n",
    "\n",
    "for col in X_train.columns:\n",
    "    # Replace infinities with NaN first\n",
    "    X_train[col] = X_train[col].replace([np.inf, -np.inf], np.nan)\n",
    "    X_val[col] = X_val[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # For numeric columns, use quantile-based clipping\n",
    "    if X_train[col].dtype in ['float64', 'float32', 'int64', 'int32']:\n",
    "        q01 = X_train[col].quantile(0.01)\n",
    "        q99 = X_train[col].quantile(0.99)\n",
    "        if not pd.isna(q01) and not pd.isna(q99) and q01 != q99:\n",
    "            X_train[col] = X_train[col].clip(lower=q01, upper=q99)\n",
    "            X_val[col] = X_val[col].clip(lower=q01, upper=q99)\n",
    "\n",
    "# Fill remaining NaN values with column means\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_val = X_val.fillna(X_train.mean())  # Use training means for validation\n",
    "\n",
    "# Final check for problematic values\n",
    "print(f\"After cleaning:\")\n",
    "print(f\"X_train infinite values: {np.isinf(X_train.values).sum()}\")\n",
    "print(f\"X_val infinite values: {np.isinf(X_val.values).sum()}\")\n",
    "print(f\"X_train NaN values: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"X_val NaN values: {X_val.isnull().sum().sum()}\")\n",
    "\n",
    "# Feature selection (with error handling)\n",
    "if len(feature_cols) > 50:\n",
    "    try:\n",
    "        print(\"Attempting feature selection...\")\n",
    "        selector = SelectKBest(score_func=f_regression, k=min(50, len(feature_cols)))\n",
    "        X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "        X_val_selected = selector.transform(X_val)\n",
    "        selected_features = X_train.columns[selector.get_support()]\n",
    "        print(f\" Selected {len(selected_features)} features out of {len(feature_cols)}\")\n",
    "        X_train = pd.DataFrame(X_train_selected, columns=selected_features, index=X_train.index)\n",
    "        X_val = pd.DataFrame(X_val_selected, columns=selected_features, index=X_val.index)\n",
    "    except Exception as e:\n",
    "        print(f\" Feature selection failed: {e}\")\n",
    "        print(\"Using all features without selection...\")\n",
    "        \n",
    "print(f\"Final feature set size: {X_train.shape[1]}\")\n",
    "print(f\"Features: {list(X_train.columns)}\")\n",
    "\n",
    "# Now continue with the rest of your notebook...\n",
    "# The scaling and sequence creation should work now\n",
    "\n",
    "# Use this instead of your current scaling section:\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Target scaling\n",
    "target_scaler = StandardScaler()\n",
    "y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "def create_sequences(X, y, sequence_length=24):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(sequence_length, len(X)):\n",
    "        X_seq.append(X[i-sequence_length:i])\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "sequence_length = 24\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "\n",
    "print(f\"\\n Final data shapes:\")\n",
    "print(f\"X_train_seq: {X_train_seq.shape}\")\n",
    "print(f\"y_train_seq: {y_train_seq.shape}\")\n",
    "print(f\"X_val_seq: {X_val_seq.shape}\")\n",
    "print(f\"y_val_seq: {y_val_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 3) (3583020122.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[149]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtarget_col = \"pm2.5_log\u001b[39m\n                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 3)\n"
     ]
    }
   ],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Target scaling (often helps with LSTM)\n",
    "target_scaler = StandardScaler()\n",
    "y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "def create_sequences(X, y, sequence_length=24):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(sequence_length, len(X)):\n",
    "        X_seq.append(X[i-sequence_length:i])\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "sequence_length = 24  # Use 24 hours of history\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, sequence_length)\n",
    "\n",
    "print(f\"Training sequences shape: {X_train_seq.shape}\")\n",
    "print(f\"Validation sequences shape: {X_val_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d488782wuR2W"
   },
   "source": [
    "# Build model\n",
    "\n",
    "Below is a simple LSTM model. Your task is to experiment with different parameters like, numbers of layers, units, activation functions, and optimizers, etc to get the best performing model. Experiment with other optimizers (e.g., SGD) or hyperparameters to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "mfx2LPHxq5fG",
    "outputId": "a5eab018-edc3-4ca5-f5f9-e896e2cbd0a1"
   },
   "outputs": [],
   "source": [
    "# def create_improved_model(input_shape):\n",
    "#     model = Sequential([\n",
    "#         LSTM(128, return_sequences=True, input_shape=input_shape),\n",
    "#         Dropout(0.2),\n",
    "#         BatchNormalization(),\n",
    "        \n",
    "#         LSTM(64, return_sequences=True),\n",
    "#         Dropout(0.2),\n",
    "#         BatchNormalization(),\n",
    "        \n",
    "#         LSTM(32),\n",
    "#         Dropout(0.2),\n",
    "#         BatchNormalization(),\n",
    "        \n",
    "#         Dense(32, activation='relu'),\n",
    "#         Dropout(0.2),\n",
    "#         Dense(16, activation='relu'),\n",
    "#         Dense(1)\n",
    "#     ])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# model = create_improved_model((sequence_length, X_train_seq.shape[2]))\n",
    "\n",
    "# # Custom optimizer with learning rate scheduling\n",
    "# optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=optimizer,\n",
    "#     loss='huber',  # Huber loss is more robust to outliers than MSE\n",
    "#     metrics=['mae']\n",
    "# )\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "\n",
    "class AdvancedEnsemble:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.weights = {}\n",
    "        self.scalers = {}\n",
    "    \n",
    "    def create_lstm_model(self, input_shape, model_type='standard'):\n",
    "        \"\"\"Create different LSTM architectures\"\"\"\n",
    "        if model_type == 'standard':\n",
    "            model = Sequential([\n",
    "                LSTM(128, return_sequences=True, input_shape=input_shape),\n",
    "                Dropout(0.3),\n",
    "                LSTM(64, return_sequences=True),\n",
    "                Dropout(0.3),\n",
    "                LSTM(32),\n",
    "                Dense(50, activation='relu'),\n",
    "                Dropout(0.2),\n",
    "                Dense(1)\n",
    "            ])\n",
    "        \n",
    "        elif model_type == 'bidirectional':\n",
    "            model = Sequential([\n",
    "                Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape),\n",
    "                Dropout(0.3),\n",
    "                Bidirectional(LSTM(32)),\n",
    "                Dense(50, activation='relu'),\n",
    "                Dropout(0.2),\n",
    "                Dense(1)\n",
    "            ])\n",
    "        \n",
    "        elif model_type == 'cnn_lstm':\n",
    "            model = Sequential([\n",
    "                Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "                MaxPooling1D(pool_size=2),\n",
    "                LSTM(64, return_sequences=True),\n",
    "                Dropout(0.3),\n",
    "                LSTM(32),\n",
    "                Dense(50, activation='relu'),\n",
    "                Dense(1)\n",
    "            ])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_tree_models(self):\n",
    "        \"\"\"Create tree-based models for ensemble\"\"\"\n",
    "        return {\n",
    "            'xgb': xgb.XGBRegressor(\n",
    "                n_estimators=1000,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42\n",
    "            ),\n",
    "            'lgb': lgb.LGBMRegressor(\n",
    "                n_estimators=1000,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            ),\n",
    "            'rf': RandomForestRegressor(\n",
    "                n_estimators=500,\n",
    "                max_depth=15,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def train_ensemble(self, X_train_seq, y_train_seq, X_val_seq, y_val_seq, \n",
    "                      X_train_flat, y_train_flat, X_val_flat, y_val_flat):\n",
    "        \"\"\"Train multiple models for ensemble\"\"\"\n",
    "        \n",
    "        # Train LSTM models\n",
    "        input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "        \n",
    "        for model_type in ['standard', 'bidirectional', 'cnn_lstm']:\n",
    "            print(f\"Training LSTM {model_type}...\")\n",
    "            model = self.create_lstm_model(input_shape, model_type)\n",
    "            model.compile(optimizer=Adam(0.001), loss='huber', metrics=['mae'])\n",
    "            \n",
    "            early_stop = EarlyStopping(patience=15, restore_best_weights=True)\n",
    "            model.fit(\n",
    "                X_train_seq, y_train_seq,\n",
    "                validation_data=(X_val_seq, y_val_seq),\n",
    "                epochs=100, batch_size=64,\n",
    "                callbacks=[early_stop], verbose=0\n",
    "            )\n",
    "            \n",
    "            self.models[f'lstm_{model_type}'] = model\n",
    "        \n",
    "        # Train tree-based models\n",
    "        tree_models = self.create_tree_models()\n",
    "        \n",
    "        for name, model in tree_models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            model.fit(X_train_flat, y_train_flat)\n",
    "            self.models[name] = model\n",
    "        \n",
    "        # Calculate ensemble weights based on validation performance\n",
    "        self.calculate_weights(X_val_seq, X_val_flat, y_val_flat)\n",
    "    \n",
    "    def calculate_weights(self, X_val_seq, X_val_flat, y_val):\n",
    "        \"\"\"Calculate optimal ensemble weights\"\"\"\n",
    "        predictions = {}\n",
    "        \n",
    "        # Get predictions from each model\n",
    "        for name, model in self.models.items():\n",
    "            if 'lstm' in name:\n",
    "                pred = model.predict(X_val_seq, verbose=0).flatten()\n",
    "            else:\n",
    "                pred = model.predict(X_val_flat)\n",
    "            \n",
    "            predictions[name] = pred\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "            print(f\"{name} RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        # Simple inverse error weighting\n",
    "        errors = {}\n",
    "        for name, pred in predictions.items():\n",
    "            errors[name] = np.sqrt(mean_squared_error(y_val, pred))\n",
    "        \n",
    "        total_inv_error = sum(1/error for error in errors.values())\n",
    "        self.weights = {name: (1/error)/total_inv_error for name, error in errors.items()}\n",
    "        \n",
    "        print(\"Ensemble weights:\", self.weights)\n",
    "    \n",
    "    def predict(self, X_seq, X_flat):\n",
    "        \"\"\"Make ensemble predictions\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            if 'lstm' in name:\n",
    "                pred = model.predict(X_seq, verbose=0).flatten()\n",
    "            else:\n",
    "                pred = model.predict(X_flat)\n",
    "            \n",
    "            predictions.append(pred * self.weights[name])\n",
    "        \n",
    "        return np.sum(predictions, axis=0)\n",
    "\n",
    "# ================================\n",
    "# 3. ADVANCED PREPROCESSING\n",
    "# ================================\n",
    "\n",
    "def create_multiple_scales(X_train, X_val, X_test):\n",
    "    \"\"\"Create data at multiple time scales\"\"\"\n",
    "    scalers = {}\n",
    "    scaled_data = {}\n",
    "    \n",
    "    # Standard scaling\n",
    "    scaler_std = StandardScaler()\n",
    "    scaled_data['standard'] = {\n",
    "        'train': scaler_std.fit_transform(X_train),\n",
    "        'val': scaler_std.transform(X_val),\n",
    "        'test': scaler_std.transform(X_test)\n",
    "    }\n",
    "    scalers['standard'] = scaler_std\n",
    "    \n",
    "    # Robust scaling\n",
    "    scaler_robust = RobustScaler()\n",
    "    scaled_data['robust'] = {\n",
    "        'train': scaler_robust.fit_transform(X_train),\n",
    "        'val': scaler_robust.transform(X_val),\n",
    "        'test': scaler_robust.transform(X_test)\n",
    "    }\n",
    "    scalers['robust'] = scaler_robust\n",
    "    \n",
    "    return scaled_data, scalers\n",
    "\n",
    "def create_multi_scale_sequences(data, sequence_lengths=[12, 24, 48]):\n",
    "    \"\"\"Create sequences of different lengths\"\"\"\n",
    "    sequences = {}\n",
    "    \n",
    "    for seq_len in sequence_lengths:\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(seq_len, len(data['X'])):\n",
    "            X_seq.append(data['X'][i-seq_len:i])\n",
    "            y_seq.append(data['y'][i])\n",
    "        \n",
    "        sequences[seq_len] = {\n",
    "            'X': np.array(X_seq),\n",
    "            'y': np.array(y_seq)\n",
    "        }\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# ================================\n",
    "# 4. ADVANCED VALIDATION STRATEGY\n",
    "# ================================\n",
    "\n",
    "def time_series_cv(train_data, n_splits=5):\n",
    "    \"\"\"Time series cross-validation\"\"\"\n",
    "    total_size = len(train_data)\n",
    "    test_size = total_size // (n_splits + 1)\n",
    "    \n",
    "    splits = []\n",
    "    for i in range(n_splits):\n",
    "        train_end = total_size - (n_splits - i) * test_size\n",
    "        test_start = train_end\n",
    "        test_end = test_start + test_size\n",
    "        \n",
    "        train_idx = list(range(0, train_end))\n",
    "        test_idx = list(range(test_start, test_end))\n",
    "        \n",
    "        splits.append((train_idx, test_idx))\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# ================================\n",
    "# 5. HYPERPARAMETER OPTIMIZATION\n",
    "# ================================\n",
    "\n",
    "def optimize_hyperparameters(X_train_seq, y_train_seq, X_val_seq, y_val_seq):\n",
    "    \"\"\"Simple grid search for key hyperparameters\"\"\"\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_params = {}\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'lstm_units': [64, 128, 256],\n",
    "        'dropout_rate': [0.2, 0.3, 0.4],\n",
    "        'learning_rate': [0.001, 0.01, 0.1],\n",
    "        'batch_size': [32, 64, 128]\n",
    "    }\n",
    "    \n",
    "    # Simple grid search (you might want to use more sophisticated methods)\n",
    "    for lstm_units in param_grid['lstm_units']:\n",
    "        for dropout_rate in param_grid['dropout_rate']:\n",
    "            for lr in param_grid['learning_rate']:\n",
    "                for batch_size in param_grid['batch_size']:\n",
    "                    \n",
    "                    model = Sequential([\n",
    "                        LSTM(lstm_units, return_sequences=True, \n",
    "                             input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
    "                        Dropout(dropout_rate),\n",
    "                        LSTM(lstm_units//2),\n",
    "                        Dense(1)\n",
    "                    ])\n",
    "                    \n",
    "                    model.compile(optimizer=Adam(lr), loss='mse')\n",
    "                    \n",
    "                    history = model.fit(\n",
    "                        X_train_seq, y_train_seq,\n",
    "                        validation_data=(X_val_seq, y_val_seq),\n",
    "                        epochs=10, batch_size=batch_size,\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    \n",
    "                    val_loss = min(history.history['val_loss'])\n",
    "                    \n",
    "                    if val_loss < best_score:\n",
    "                        best_score = val_loss\n",
    "                        best_params = {\n",
    "                            'lstm_units': lstm_units,\n",
    "                            'dropout_rate': dropout_rate,\n",
    "                            'learning_rate': lr,\n",
    "                            'batch_size': batch_size\n",
    "                        }\n",
    "    \n",
    "    print(\"Best parameters:\", best_params)\n",
    "    return best_params\n",
    "\n",
    "# ================================\n",
    "# 6. POST-PROCESSING TECHNIQUES\n",
    "# ================================\n",
    "\n",
    "def post_process_predictions(predictions, y_train):\n",
    "    \"\"\"Apply post-processing to improve predictions\"\"\"\n",
    "    \n",
    "    # Clip to reasonable range based on training data\n",
    "    train_min, train_max = y_train.min(), y_train.max()\n",
    "    predictions = np.clip(predictions, train_min * 0.5, train_max * 1.5)\n",
    "    \n",
    "    # Apply smoothing to reduce noise\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    predictions_smooth = gaussian_filter1d(predictions, sigma=1.0)\n",
    "    \n",
    "    # Blend original and smoothed predictions\n",
    "    predictions_final = 0.7 * predictions + 0.3 * predictions_smooth\n",
    "    \n",
    "    return predictions_final\n",
    "\n",
    "# ================================\n",
    "# 7. MAIN TRAINING PIPELINE\n",
    "# ================================\n",
    "\n",
    "def train_advanced_model(train, test, target_scaler):\n",
    "    \"\"\"Complete advanced training pipeline\"\"\"\n",
    "    \n",
    "    # 1. Advanced feature engineering\n",
    "    print(\"Creating advanced features...\")\n",
    "    train_advanced = create_advanced_features(train)\n",
    "    test_advanced = create_advanced_features(test)\n",
    "    \n",
    "    # 2. Data preparation\n",
    "    val_size = int(len(train_advanced) * 0.2)\n",
    "    train_data = train_advanced.iloc[:-val_size]\n",
    "    val_data = train_advanced.iloc[-val_size:]\n",
    "    \n",
    "    feature_cols = [col for col in train_advanced.columns if col not in ['pm2.5', 'No']]\n",
    "    \n",
    "    X_train = train_data[feature_cols]\n",
    "    y_train = train_data['pm2.5']\n",
    "    X_val = val_data[feature_cols]\n",
    "    y_val = val_data['pm2.5']\n",
    "    \n",
    "    # Clean data\n",
    "    for col in X_train.columns:\n",
    "        X_train[col] = X_train[col].replace([np.inf, -np.inf], np.nan)\n",
    "        X_val[col] = X_val[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_val = X_val.fillna(X_train.mean())\n",
    "    \n",
    "    # 3. Multiple scaling strategies\n",
    "    X_test = test_advanced[feature_cols]\n",
    "    X_test = X_test.fillna(X_train.mean())\n",
    "    \n",
    "    scaled_data, scalers = create_multiple_scales(X_train, X_val, X_test)\n",
    "    \n",
    "    # Use robust scaling for main model\n",
    "    X_train_scaled = scaled_data['robust']['train']\n",
    "    X_val_scaled = scaled_data['robust']['val']\n",
    "    X_test_scaled = scaled_data['robust']['test']\n",
    "    \n",
    "    # Scale targets\n",
    "    y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    y_val_scaled = target_scaler.transform(y_val.values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # 4. Create sequences\n",
    "    def create_sequences(X, y, seq_len=24):\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(seq_len, len(X)):\n",
    "            X_seq.append(X[i-seq_len:i])\n",
    "            y_seq.append(y[i])\n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "    \n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled)\n",
    "    X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled)\n",
    "    \n",
    "    # Also prepare flat data for tree models\n",
    "    X_train_flat = X_train_scaled[24:]  # Skip first 24 to match sequence data\n",
    "    X_val_flat = X_val_scaled[24:]\n",
    "    y_train_flat = y_train_scaled[24:]\n",
    "    y_val_flat = y_val_scaled[24:]\n",
    "    \n",
    "    # 5. Train ensemble\n",
    "    ensemble = AdvancedEnsemble()\n",
    "    ensemble.train_ensemble(\n",
    "        X_train_seq, y_train_seq, X_val_seq, y_val_seq,\n",
    "        X_train_flat, y_train_flat, X_val_flat, y_val_flat\n",
    "    )\n",
    "    \n",
    "    # 6. Make test predictions\n",
    "    def create_test_sequences(X_test_scaled, X_train_scaled, seq_len=24):\n",
    "        combined = np.vstack([X_train_scaled[-seq_len:], X_test_scaled])\n",
    "        sequences = []\n",
    "        for i in range(seq_len, len(combined)):\n",
    "            sequences.append(combined[i-seq_len:i])\n",
    "        return np.array(sequences)\n",
    "    \n",
    "    X_test_seq = create_test_sequences(X_test_scaled, X_train_scaled)\n",
    "    X_test_flat = X_test_scaled\n",
    "    \n",
    "    test_pred_scaled = ensemble.predict(X_test_seq, X_test_flat)\n",
    "    test_pred = target_scaler.inverse_transform(test_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # 7. Post-processing\n",
    "    test_pred = post_process_predictions(test_pred, y_train.values)\n",
    "    test_pred = np.clip(test_pred, 0, 500).astype(int)\n",
    "    \n",
    "    return test_pred, ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uM0Xuq7XvdTZ",
    "outputId": "b6df9dee-acfd-416b-d50e-ab9b40201c73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 42ms/step - loss: 0.0368 - mae: 0.1819 - val_loss: 0.0573 - val_mae: 0.2527 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 43ms/step - loss: 0.0355 - mae: 0.1787 - val_loss: 0.0682 - val_mae: 0.2726 - learning_rate: 5.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 42ms/step - loss: 0.0355 - mae: 0.1772 - val_loss: 0.0634 - val_mae: 0.2681 - learning_rate: 5.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m200/383\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - loss: 0.0332 - mae: 0.1757"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=8,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, lr_reducer],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "train_pred_scaled = model.predict(X_train_seq)\n",
    "val_pred_scaled = model.predict(X_val_seq)\n",
    "\n",
    "# Inverse transform predictions\n",
    "train_pred = target_scaler.inverse_transform(train_pred_scaled).flatten()\n",
    "val_pred = target_scaler.inverse_transform(val_pred_scaled).flatten()\n",
    "\n",
    "# Get actual values (accounting for sequence offset)\n",
    "y_train_actual = y_train.iloc[sequence_length:].values\n",
    "y_val_actual = y_val.iloc[sequence_length:].values\n",
    "\n",
    "# Calculate metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual, train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val_actual, val_pred))\n",
    "val_mae = mean_absolute_error(y_val_actual, val_pred)\n",
    "\n",
    "# Persistence baseline\n",
    "persistence_pred = y_val.shift(1).iloc[sequence_length:].values\n",
    "persistence_rmse = np.sqrt(mean_squared_error(y_val_actual, persistence_pred))\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training history\n",
    "axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0, 0].set_title('Training History')\n",
    "axes[0, 0].set_xlabel('Epochs')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Validation predictions vs actual\n",
    "axes[0, 1].scatter(y_val_actual, val_pred, alpha=0.5)\n",
    "axes[0, 1].plot([y_val_actual.min(), y_val_actual.max()], \n",
    "                [y_val_actual.min(), y_val_actual.max()], 'r--', lw=2)\n",
    "axes[0, 1].set_xlabel('Actual PM2.5')\n",
    "axes[0, 1].set_ylabel('Predicted PM2.5')\n",
    "axes[0, 1].set_title('Validation: Predicted vs Actual')\n",
    "\n",
    "# Time series plot\n",
    "axes[1, 0].plot(y_val_actual[:200], label='Actual', alpha=0.7)\n",
    "axes[1, 0].plot(val_pred[:200], label='Predicted', alpha=0.7)\n",
    "axes[1, 0].set_title('Time Series Comparison (First 200 points)')\n",
    "axes[1, 0].set_xlabel('Time')\n",
    "axes[1, 0].set_ylabel('PM2.5')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Residuals\n",
    "residuals = y_val_actual - val_pred\n",
    "axes[1, 1].hist(residuals, bins=30, alpha=0.7)\n",
    "axes[1, 1].set_title('Residuals Distribution')\n",
    "axes[1, 1].set_xlabel('Residual')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training RMSE: {train_rmse:.2f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.2f}\")\n",
    "print(f\"Validation MAE: {val_mae:.2f}\")\n",
    "print(f\"Persistence Baseline RMSE: {persistence_rmse:.2f}\")\n",
    "print(f\"Improvement over baseline: {((persistence_rmse - val_rmse) / persistence_rmse * 100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_window_test(test_df, lookback):\n",
    "    X_test = []\n",
    "    # Use the same feature selection as training (drop 'No' and 'pm2.5' if present)\n",
    "    features_to_drop = ['No']\n",
    "    if 'pm2.5' in test_df.columns:\n",
    "        features_to_drop.append('pm2.5')\n",
    "    test_values = test_df.drop(features_to_drop, axis=1).values\n",
    "    \n",
    "    for i in range(lookback, len(test_df)):\n",
    "        X_test.append(test_values[i-lookback:i])\n",
    "    return np.array(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_predictions(test_df, model, lookback):\n",
    "    # Drop non-feature columns\n",
    "    test_features = test_df.drop(['No'], axis=1, errors='ignore')\n",
    "    if 'pm2.5' in test_features.columns:\n",
    "        test_features = test_features.drop(['pm2.5'], axis=1)\n",
    "\n",
    "    values = test_features.values\n",
    "    n = len(values)\n",
    "    preds = []\n",
    "\n",
    "    # Pad the first `lookback` rows with the first valid window prediction\n",
    "    first_window = values[:lookback].reshape(1, lookback, -1)\n",
    "    first_pred = model.predict(first_window, verbose=0)[0, 0]\n",
    "    preds.extend([first_pred] * lookback)\n",
    "\n",
    "    # Build sliding windows for the rest (batch all at once)\n",
    "    windows = np.array([\n",
    "        values[i - lookback:i] for i in range(lookback, n)\n",
    "    ])\n",
    "    batch_preds = model.predict(windows, verbose=0).flatten()\n",
    "    preds.extend(batch_preds.tolist())\n",
    "\n",
    "    return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nrw_e7OVwe6R",
    "outputId": "9a7966e6-fccf-409e-b3e4-c6ba968d610e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test sequences...\n",
      "Adding 41 missing features to test data\n",
      "Test data shape before scaling: (13148, 50)\n",
      "Test sequences shape: (13148, 24, 50)\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step\n",
      "Test predictions - Min: 26, Max: 214, Mean: 33.4\n",
      "Submission shape: (13148, 2)\n",
      "               row ID  pm2.5\n",
      "0  2013-07-02 4:00:00    214\n",
      "1  2013-07-02 5:00:00     72\n",
      "2  2013-07-02 6:00:00     26\n",
      "3  2013-07-02 7:00:00     27\n",
      "4  2013-07-02 8:00:00     29\n"
     ]
    }
   ],
   "source": [
    "def create_test_sequences_simple(test_df, X_train_scaled, scaler, sequence_length, selected_features):\n",
    "    \n",
    "    test_prep = test_df.copy()\n",
    "    \n",
    "    # Add missing features that were selected during training\n",
    "    missing_features = [col for col in selected_features if col not in test_prep.columns]\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"Adding {len(missing_features)} missing features to test data\")\n",
    "        \n",
    "        # Use simple defaults for missing features\n",
    "        for col in missing_features:\n",
    "            if 'pm2.5' in col:\n",
    "                if 'lag' in col:\n",
    "                    test_prep[col] = 50  # Reasonable PM2.5 default\n",
    "                elif 'rolling' in col:\n",
    "                    test_prep[col] = 50\n",
    "                elif 'ewm' in col:\n",
    "                    test_prep[col] = 50\n",
    "                elif 'pct_change' in col:\n",
    "                    test_prep[col] = 0\n",
    "                else:\n",
    "                    test_prep[col] = 0\n",
    "            else:\n",
    "                test_prep[col] = 0\n",
    "    \n",
    "    # Select only the features that were used in training\n",
    "    X_test = test_prep[selected_features]\n",
    "    \n",
    "    # Clean test data\n",
    "    for col in X_test.columns:\n",
    "        X_test[col] = X_test[col].replace([np.inf, -np.inf], np.nan)\n",
    "        X_test[col] = X_test[col].fillna(X_test[col].mean() if X_test[col].mean() == X_test[col].mean() else 0)\n",
    "    \n",
    "    print(f\"Test data shape before scaling: {X_test.shape}\")\n",
    "    \n",
    "    # Scale test features\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Use the last sequence_length rows from training data\n",
    "    train_tail = X_train_scaled[-sequence_length:]\n",
    "    \n",
    "    # Combine training tail with test data\n",
    "    combined_data = np.vstack([train_tail, X_test_scaled])\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = []\n",
    "    for i in range(sequence_length, len(combined_data)):\n",
    "        sequences.append(combined_data[i-sequence_length:i])\n",
    "    \n",
    "    return np.array(sequences)\n",
    "\n",
    "# Use the simplified function\n",
    "print(\"Creating test sequences...\")\n",
    "X_test_seq = create_test_sequences_simple(\n",
    "    test, \n",
    "    X_train_scaled,  # Use the already scaled training data\n",
    "    scaler, \n",
    "    sequence_length, \n",
    "    selected_features=X_train.columns.tolist()  # Features that were actually used\n",
    ")\n",
    "\n",
    "print(f\"Test sequences shape: {X_test_seq.shape}\")\n",
    "\n",
    "# Make predictions\n",
    "test_pred_scaled = model.predict(X_test_seq, verbose=1)\n",
    "test_pred = target_scaler.inverse_transform(test_pred_scaled).flatten()\n",
    "\n",
    "# Ensure reasonable predictions\n",
    "test_pred = np.clip(test_pred, 0, 500)\n",
    "test_pred = np.round(test_pred).astype(int)\n",
    "\n",
    "print(f\"Test predictions - Min: {test_pred.min()}, Max: {test_pred.max()}, Mean: {test_pred.mean():.1f}\")\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'row ID': test.index.strftime('%Y-%m-%d %-H:%M:%S'),\n",
    "    'pm2.5': test_pred\n",
    "})\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "submission.to_csv('data/final_submission.csv', index=False)\n",
    "\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
